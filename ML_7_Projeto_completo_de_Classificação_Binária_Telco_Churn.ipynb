{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itaborai83/ecd221-ML-trabalho/blob/main/ML_7_Projeto_completo_de_Classifica%C3%A7%C3%A3o_Bin%C3%A1ria_Telco_Churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otEdveLq8Hn0"
      },
      "source": [
        "# Especialização em Ciência de Dados - PUC-Rio\n",
        "# Machine Learning - Prof. Tatiana Escovedo\n",
        "# Projeto completo de Classificação Binária\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1PEQEdZ9zm4"
      },
      "source": [
        "## 1. Definição do Problema"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDcdO4yx9db6"
      },
      "source": [
        "https://www.kaggle.com/datasets/blastchar/telco-customer-churn\n",
        "\n",
        "**Informações sobre os atributos:**\n",
        "\n",
        "Adicionar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGuBBNa_DLat",
        "outputId": "fe7637f1-79f3-4683-fd6a-3b18d1db4d94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting phik\n",
            "  Downloading phik-0.12.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (690 kB)\n",
            "\u001b[K     |████████████████████████████████| 690 kB 14.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: mlxtend in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: scipy>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from phik) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.7/dist-packages (from phik) (1.1.0)\n",
            "Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.7/dist-packages (from phik) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from phik) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from phik) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->phik) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->phik) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->phik) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->phik) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2.3->phik) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->phik) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2.3->phik) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from mlxtend) (57.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from mlxtend) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->mlxtend) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "# função de correlação que funciona com variáveis categóricas e numéricas\n",
        "! pip install phik mlxtend\n",
        "\n",
        "# https://stackoverflow.com/questions/61867945/python-import-error-cannot-import-name-six-from-sklearn-externals\n",
        "! pip install six "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCn8CH4M7wF-"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import sys\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as ms # para tratamento de missings\n",
        "from matplotlib import cm\n",
        "from pandas import set_option\n",
        "from pandas.plotting import scatter_matrix\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer # transformador de colunas, usado para tratamento das variáveis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# a versão do Stacking classifier disponível no Colab não aceita classificadores pré-treinados\n",
        "# from sklearn.ensemble import StackingClassifier\n",
        "# correção de bug na biblioteca que importa uma dependência de maneira indireta\n",
        "# https://stackoverflow.com/questions/61867945/python-import-error-cannot-import-name-six-from-sklearn-externals\n",
        "import six\n",
        "sys.modules['sklearn.externals.six'] = six\n",
        "from mlxtend.classifier import EnsembleVoteClassifier\n",
        "\n",
        "import phik\n",
        "from phik.report import plot_correlation_matrix\n",
        "from phik import report\n",
        "\n",
        "# redução de dimensionalidade\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# para baixar os modelos já treinados\n",
        "import urllib.request\n",
        "\n",
        "from pprint import pprint, pformat\n",
        "\n",
        "import pickle # para persistência de modelos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJpWLh52-aPE"
      },
      "outputs": [],
      "source": [
        "# setup ambiente\n",
        "\n",
        "# supressão de warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# configura pandas para exibição de apenas duas casas decimais nas variaveis\n",
        "pd.set_option('display.float_format', lambda x: '%0.2f' % x)\n",
        "\n",
        "# configura cores do Seaborn\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVW-_y-SXZ0x"
      },
      "outputs": [],
      "source": [
        "# constantes\n",
        "\n",
        "DATA_URL                    = f\"https://raw.githubusercontent.com/itaborai83/ecd221-ML-trabalho/main/telco-churn.csv\"\n",
        "FIELD_SEPARATOR             = \",\"\n",
        "IMPORT_COLUMN_NAMES         = [\n",
        "    \"customer_id\"\n",
        ",   \"gender\"\n",
        ",   \"senior_citizen\"\n",
        ",   \"partner\"\n",
        ",   \"dependents\"\n",
        ",   \"tenure\"\n",
        ",   \"phone_service\"\n",
        ",   \"multiple_lines\"\n",
        ",   \"internet_service\"\n",
        ",   \"online_security\"\n",
        ",   \"online_backup\"\n",
        ",   \"device_protection\"\n",
        ",   \"tech_support\"\n",
        ",   \"streaming_tv\"\n",
        ",   \"streaming_movies\"\n",
        ",   \"contract\"\n",
        ",   \"paperless_billing\"\n",
        ",   \"payment_method\"\n",
        ",   \"monthly_charges\"\n",
        ",   \"total_charges\"\n",
        ",   \"churn\"\n",
        "]\n",
        "COLUMN_NAMES = [\n",
        "    \"gender\"\n",
        ",   \"senior_citizen\"\n",
        ",   \"partner\"\n",
        ",   \"dependents\"\n",
        ",   \"phone_service\"\n",
        ",   \"multiple_lines\"\n",
        ",   \"internet_service\"\n",
        ",   \"online_security\"\n",
        ",   \"online_backup\"\n",
        ",   \"device_protection\"\n",
        ",   \"tech_support\"\n",
        ",   \"streaming_tv\"\n",
        ",   \"streaming_movies\"\n",
        ",   \"contract\"\n",
        ",   \"paperless_billing\"\n",
        ",   \"payment_method\"\n",
        ",   \"tenure\"\n",
        ",   \"monthly_charges\"\n",
        ",   \"total_charges\"\n",
        ",   \"churn\"\n",
        "]\n",
        "TARGET_VARIABLE = \"churn\"\n",
        "BOOLEAN_FEATURES = [\n",
        "    \"senior_citizen\"\n",
        ",   \"partner\"\n",
        ",   \"dependents\"\n",
        ",   \"phone_service\"\n",
        ",   \"paperless_billing\"\n",
        "]\n",
        "CATEGORICAL_FEATURES = [\n",
        "    \"multiple_lines\"\n",
        ",   \"internet_service\"\n",
        ",   \"online_security\"\n",
        ",   \"online_backup\"\n",
        ",   \"device_protection\"\n",
        ",   \"tech_support\"\n",
        ",   \"streaming_tv\"\n",
        ",   \"streaming_movies\"\n",
        ",   \"contract\"\n",
        ",   \"payment_method\"\n",
        "]\n",
        "NUMERICAL_FEATURES                  = [\"tenure\", \"monthly_charges\", \"total_charges\"]\n",
        "NUMERICAL_FEATURES_AFTER_FEAT_ENG   = [\"tenure\", \"monthly_charges\", \"total_charges\", \"client_factor\", \"internet_factor\", \"financial_factor\", \"multi_factor\"]\n",
        "BOOLEAN_MAP                         = {\"No\": 0, \"Yes\": 1}\n",
        "TEST_PCT_SIZE                       = 0.3 # 30% do conjunto de dados\n",
        "RANDOM_STATE                        = 42\n",
        "SCORING_METRIC                      = \"roc_auc\"\n",
        "K_FOLDS                             = 6\n",
        "RETURN_TRAIN_SCORE                  = False\n",
        "OUTPUT_TRAINING_FILE_TMPLT          = \"RandomizedSearchCV_{algo}.xlsx\"\n",
        "QUICK_RUN                           = False\n",
        "DOWNLOAD_MODELS_URL                 = \"https://github.com/itaborai83/ecd221-ML-trabalho/raw/main/\"\n",
        "MODEL_SETTINGS                      = {\n",
        "     \"logreg\"           : { \"model_file\": \"logreg.pkl\", \"train\": True, \"download\": False }\n",
        ",    \"knn\"              : { \"model_file\": \"knn.pkl\",    \"train\": True, \"download\": False }\n",
        ",    \"nb\"               : { \"model_file\": \"nb.pkl\",     \"train\": True, \"download\": False }\n",
        ",    \"dt\"               : { \"model_file\": \"dt.pkl\",     \"train\": True, \"download\": False }\n",
        ",    \"svm\"              : { \"model_file\": \"svm.pkl\",    \"train\": True, \"download\": False }\n",
        ",    \"ada\"              : { \"model_file\": \"ada.pkl\",    \"train\": True, \"download\": False }\n",
        ",    \"gb\"               : { \"model_file\": \"gb.pkl\",     \"train\": True, \"download\": False }\n",
        ",    \"rf\"               : { \"model_file\": \"rf.pkl\",     \"train\": True, \"download\": False }\n",
        ",    \"vt\"               : { \"model_file\": \"vt.pkl\",     \"train\": True, \"download\": False }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PcB0Efd-MS4"
      },
      "source": [
        "## 2. Carga de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmHBahfF-cJL"
      },
      "source": [
        "Iremos usar o pacote Pandas ( Python Data Analysis Library) para carregar de um arquivo .csv sem cabeçalho disponível online.\n",
        "\n",
        "Com o dataset carregado, iremos explorá-lo um pouco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29AFuCPtvG_i"
      },
      "outputs": [],
      "source": [
        "# Carrega arquivo csv usando Pandas usando uma URL\n",
        "churn_df = pd.read_csv(\n",
        "    DATA_URL\n",
        ",   names     = IMPORT_COLUMN_NAMES\n",
        ",   skiprows  = 1\n",
        ",   delimiter = ','\n",
        ")\n",
        "\n",
        "# transforma a variável target em uma variável numérica\n",
        "churn_df[\"churn\"] = churn_df[\"churn\"].map(BOOLEAN_MAP)\n",
        "\n",
        "# excluindo a variável customer_id\n",
        "del churn_df[\"customer_id\"]\n",
        "\n",
        "for algo in MODEL_SETTINGS:\n",
        "  model_setting = MODEL_SETTINGS[algo]\n",
        "  download      = model_setting[\"download\"]\n",
        "  model_file    = model_setting[\"model_file\"]\n",
        "  if download:\n",
        "    url = DOWNLOAD_MODELS_URL + model_file\n",
        "    urllib.request.urlretrieve(url, model_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxWtav9TyS1R"
      },
      "outputs": [],
      "source": [
        "churn_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBSgQt_z_TnV"
      },
      "source": [
        "## 3. Análise de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqINv-wo_Xfe"
      },
      "source": [
        "### 3.1. Estatísticas Descritivas\n",
        "\n",
        "Vamos iniciar examinando as dimensões do dataset, suas informações e alguns exemplos de linhas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF3f00Zl_g7j"
      },
      "outputs": [],
      "source": [
        "# Mostra as dimensões do dataset\n",
        "print(churn_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUjmUTEOwQZt"
      },
      "outputs": [],
      "source": [
        "# Mostra as informações do dataset\n",
        "print(churn_df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWJsBu2EtrAm"
      },
      "source": [
        "o dataset possui predominantesmente dados categóricos, representados acima como objetos com tipo de dados **object**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPy9N4FuwX6H"
      },
      "outputs": [],
      "source": [
        "# Mostra as 10 primeiras linhas do dataset\n",
        "churn_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NmYccn7vZq7"
      },
      "source": [
        "A variável **monthly_charges** aparenta ser numérica.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOuh1A0Zve67"
      },
      "outputs": [],
      "source": [
        "# a conversão do tipo da coluna abaixo falha com o erro \"ValueError: could not convert string to float\"\n",
        "# churn_df[\"total_charges\"] = churn_df[\"total_charges\"].astype(float)\n",
        "\n",
        "# tenta identificar valores problemáticos\n",
        "def convertible_to_float(value):\n",
        "  try:\n",
        "    f = float(value)\n",
        "    return True\n",
        "  except:\n",
        "    return False\n",
        "is_float = churn_df[\"total_charges\"].map(convertible_to_float)\n",
        "churn_df[\"total_charges_is_float\"] = is_float\n",
        "display(churn_df[ churn_df[\"total_charges_is_float\"] == False ][ \"total_charges\" ].map(repr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PffckM8nx6MJ"
      },
      "source": [
        "O dataframe possui 11 registros com valor ' ' na coluna total_charges. Os valores serã convertidos para zero e posteriormente analisados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz7DTgc_yMK3"
      },
      "outputs": [],
      "source": [
        "def convert_total_charges(value):\n",
        "    return 0.0 if value == ' ' else value\n",
        "\n",
        "churn_df[\"total_charges\"] = churn_df[\"total_charges\"].map(convert_total_charges).astype(float)\n",
        "del churn_df[\"total_charges_is_float\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMyYMlMVwYCS"
      },
      "outputs": [],
      "source": [
        "# Mostra as 10 últimas linhas do dataset\n",
        "churn_df.tail(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdR22S5G0i6d"
      },
      "source": [
        "Diferente das outras variáveis categóricas, a variável **senior_citizem** está utilizando 0's e 1's para representar verdadeiro e falso. Iremos converter estes dados momentamenteamente por questões de consistência"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVm6n1SY0xhx"
      },
      "outputs": [],
      "source": [
        "churn_df[\"senior_citizen\"] = churn_df[\"senior_citizen\"].map({1: \"Yes\", 0: \"No\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY7gaBfqwdy5"
      },
      "source": [
        "É sempre importante verificar o tipo do atributos do dataset, pois pode ser necessário realizar conversões. Já fizemos anteriormente com o comando info, mas vamos ver uma outra forma de verificar a natureza de cada atributo e então exibir um resumo estatístico do dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dRVheWE_mJQ"
      },
      "outputs": [],
      "source": [
        "# Verifica o tipo de dataset de cada atributo\n",
        "churn_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NKdYewownzG"
      },
      "outputs": [],
      "source": [
        "# Faz um resumo estatístico das variáveis numéricas do dataset (média, desvio padrão, mínimo, máximo e os quartis)\n",
        "churn_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7YMO2cIaQlx"
      },
      "source": [
        "a variável **total_charges** é numericamente muito próxima das variáveis **tenure** e **monthly_charges** multiplicadas e por isso será removida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHkPGLyPZ4oX"
      },
      "outputs": [],
      "source": [
        "churn_df[\"tenure*monthly\"] = churn_df[\"tenure\"] * churn_df[\"monthly_charges\"]\n",
        "churn_df[\"churn\"] = churn_df.pop(\"churn\")\n",
        "display(churn_df.describe())\n",
        "del churn_df[\"tenure*monthly\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5gT8_cH1WKk"
      },
      "source": [
        "O dataset não possui valores faltantes ou valores anormalmente altos ou baixos em suas variaveis numéricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fabLTz9c_tJE"
      },
      "source": [
        "Vamos agora verificar se o dataset tem as classes balanceadas para que possamos tratar o desbalanceamento posteriormente, se necessário. Veremos que as classes 0 (permaneceu cliente) e 1 (não permaneceu cliente) estão desbalanceadas. Vamos guardar esta informação, pois possivelmente precisaremos realizar algum tipo de tratamento nas próximas etapas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5aRF4dAtWda"
      },
      "outputs": [],
      "source": [
        "# distribuição das classes\n",
        "pd.crosstab(churn_df[TARGET_VARIABLE], churn_df[TARGET_VARIABLE], normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GZ8ORQ82cq3"
      },
      "source": [
        "Não está claro se o balanceamento de classes e a distribuição das variáveis do dataset correspondem à dados observados na vida real ou se algum tipo de curadoria ou rebalanceamento foi realizado (dado que a rotatividade de clientes na ordem de 27% parece ser um percentual muito elevado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCwB5SZyH3fB"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"gender\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlqaBobXtBpF"
      },
      "source": [
        "A variável **gender** está balanceada no dataset e também balanceado quando comparada à variável target. Não está claro se essa variável é realmente útil."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEK5_C2vtUIV"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"senior_citizen\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQgNB4ZBtZYT"
      },
      "source": [
        "A variável categórica **senior_citizen** parece influenciar a rotatividade de clientes.\n",
        "\n",
        "Se por um lado 84% dos clientes não estão na terceira idade, aproximadamente 75% (20%/27%) dos ex-clientes pertencem à esta categoria.\n",
        "\n",
        "Por sua vez, os clientes pertencentes à terceira idade correspondem a apenas 16% dos clientes, mas desses 16%, 43% deixaram de ser clientes.\n",
        "\n",
        "Em outras palavras. Em termos absolutos, um ex-cliente tem maior probabilidade de não pertencer a terceira idade. Em contra-partida, dado que um cliente pertence à terceira idade, ele tem maior probabilidade de se tornar um ex-cliente.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqiM6MVX4vEC"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"partner\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScYqv00w5gkC"
      },
      "source": [
        "A variável **partner**, que indica se um cliente possui ou não algum tipo de parceiro/cônjuge parece estar bem distribuída no conjunto de dados com aproximadamente metade dos clientes tendo algum parceiro e a outra metade não.\n",
        "\n",
        "Entretanto, podemos que observar que a retenção dos clientes tende a ser pior entre os clientes que não possuem algum tipo de parceiro, pois aproximadamente 2/3 dos ex-clientes enquadram-se nessa situação"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_Og4eVp_eNN"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"dependents\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX6gXl0i_lzS"
      },
      "source": [
        "A variável **dependents**, que indica se um cliente possui ou não algum tipo de dependente parece ser um forte indicador sobre a rotatividade dos clientes pois mais de 81% (22%/27%) dos ex-clientes não possuem dependentes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plf9wQo4A_dc"
      },
      "outputs": [],
      "source": [
        "g = sns.FacetGrid(churn_df, col=\"churn\", size=3)\n",
        "g.map_dataframe(plt.hist, x=\"tenure\", alpha=0.5)\n",
        "g.add_legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlcB2eGY77jG"
      },
      "source": [
        "A variável **tenure**, que representa o número de meses em que um cliente possui algum tipo de relacionamento comercial com a empresa, parece indicar que a empresa oferece planos/contratos de duração máxima de 72 meses. \n",
        "\n",
        "Analisando os clientes que permaneceram com a empresa, podemos obseravar uma distribuição em forma de U, onde um número sigificativo de clientes possuem poucos meses de relacionamento e um número ainda maior parece estar se aproximando do teto de 72 meses.\n",
        "\n",
        "Por sua vez, a análise dos futuros ex-clientes indicam que a maioria parece cancelar seus serviços nos primeiros meses de relacionamento com a empresa e, passado o tempo, fidelizam-se.\n",
        "\n",
        "Estas distribuições parecem indicar a necessidade de que medidas de fidelização sejam tomadas nos críticos primeiros meses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RD0he6Dx_ly3"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"phone_service\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duki-jbp_zUR"
      },
      "source": [
        "A variável **phone_service** está extremamente desbalanceada e o fato de um usuário não possuir nenhum tipo de serviço telefônico contratado parece não ser expressivo em termos absolutos ou relativos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXkOcHMeASf3"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"multiple_lines\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXQVe6kRBnZX"
      },
      "source": [
        "A variável **multiple_lines** parece não informações significativas para determinar a rotatividade dos clientes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mdl4Be-SFoJy"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"internet_service\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQDpkoLZF5Z9"
      },
      "source": [
        "A variável **internet_service** usada para identificar qual o tipo de fornecimento de internet o usuário possui (ou não) indica que quase 2/3 entre todos os ex-clientes parecem ter contratado internet por fibra ótica.\n",
        "\n",
        "Isso parece indicar a existência de problemas na qualidade do fornecimento do serviço ou na precificação do mesmo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDsC8BYOHEU9"
      },
      "outputs": [],
      "source": [
        "g = sns.FacetGrid(churn_df, col=\"internet_service\", row=\"churn\", size=3)\n",
        "g.map_dataframe(plt.hist, x=\"monthly_charges\", alpha=0.5)\n",
        "g.add_legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pQkOZOaIGVn"
      },
      "source": [
        "O serviço de fibra ótica, conforme esperado, parece contribuir com o aumento dos custos mensais dos clientes e consequentemente afetando negativamente na retenção dos clientes que contrataram esse serviço."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XdZQSTqI2td"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"online_security\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjuCPYSDJBkE"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"online_backup\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xlhn5mI2JG5b"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"device_protection\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5h2JaM0iJMVh"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"tech_support\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuXTkYcPJkOr"
      },
      "source": [
        "As variáveis **online_security**, **online_backup**, **device_protection** e **tech_support** comportam-se de maneira parecida. Em todos os casos, a não contratação dos serviços parecem aumentar a probabilidade do cliente desfazer seu relacionamento com a empresa.\n",
        "\n",
        "Em outras palavras, a contratação desses serviços parece ser indicativo de que um cliente está fidelizado.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whn7JnpJqkEc"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"streaming_tv\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSOi4LIMJbOt"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"streaming_movies\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uvB-JTFMxnw"
      },
      "source": [
        "As variáveis **streaming_movies** e **streaming_tv** parecem não influenciar de forma significativa a rotatividade dos clientes, possivelmente por se tratarem de serviços de uso muito prevalente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8Ux1XGvtIhZ"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"contract\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfbEpgkgOrFi"
      },
      "source": [
        "Aproximadamente 85% da rotatividade dos clientes ocorrem entre aqueles que possuem contratos mensais, aparentando-ser um dos principais fatores que contribuem com a rotatividade dos clientes.\n",
        "\n",
        "Os clientes tendem a permanecer fidelizados pela duração do contrato vigente que assinaram com a empresa de telefonia/telecomunicações."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CipHBl2CPdX0"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"paperless_billing\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnsfuCzNPwup"
      },
      "source": [
        "Clientes que possuem cobrança digital tendem a ter uma rotatividade mais alta do que clientes cobrados de maneira impressa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8TFBdDlQ17O"
      },
      "outputs": [],
      "source": [
        "pd.crosstab(churn_df[\"payment_method\"], churn_df[TARGET_VARIABLE], normalize=True, margins=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCLJ5lTPRQxM"
      },
      "source": [
        "os métodos de pagamento presentes no dataset estão todos balanceados quando compara-se os cliente que permaneceram contratando os serviços da empresa.\n",
        "\n",
        "Entre os ex-clientes, a probabilidade de que o método de pagamento empregado tenha sido cheques eletrônicos (depósitos automáticos) é consideravelmente maior, conforme dados acima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8tPPwc7SXl6"
      },
      "outputs": [],
      "source": [
        "g = sns.FacetGrid(churn_df, hue=\"churn\", size=3)\n",
        "g.map_dataframe(plt.hist, x=\"monthly_charges\", alpha=0.5)\n",
        "g.add_legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBkXWNxySsJG"
      },
      "source": [
        "Conforme analisada realizada anteriormente, a variável **monthly_charges** parece ter uma significativa correlação positiva com a rotatividade dos clientes. \n",
        "\n",
        "Foi observado que o serviço contratado de internet de fibra ótica parece ser, dentre os serviços existentes, aquele que mais contribui com o aumento do valor mensal cobrado e que, consequentemente, mais influencia na rotatividade.\n",
        "\n",
        "Ademais, os dados dos ex-clientes parecem indicar a existência de uma distribuição tri-modal composta primeiramente pelos clientes que utilizam-se apenas dos serviços de telefonia, subsequentemente, os clientes que utilizam-se da internet via DSL e, por último, os clientes que se utilizam da internet via fibra ótica."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fXFgfdgmWeT"
      },
      "source": [
        "Com base nas análises acima, as seguintes ações serão tomadas\n",
        "\n",
        "*   Variável **gender** será eliminada;\n",
        "*   Variável **total_charges** será elimnada;\n",
        "*   As variáveis categoricas serão posteriormente tratadas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSg9Mup4nulw"
      },
      "outputs": [],
      "source": [
        "del churn_df[\"gender\"]\n",
        "churn_df[\"total_charges\"] = churn_df[\"monthly_charges\"] * churn_df[\"tenure\"]\n",
        "churn_df[\"churn\"] = churn_df.pop(\"churn\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_5Ntt3f_tTv"
      },
      "source": [
        "### 3.2. Visualizações Unimodais Dados Numéricos\n",
        "\n",
        "Vamos criar agora um histograma para cada atributo do dataset. Veremos que os atributos age, pedi e test seguem uma distribuição exponencial, e que as colunas mass e press seguem uma distribuição aproximadamente normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK3mK65T_tYx"
      },
      "outputs": [],
      "source": [
        "# Histograma\n",
        "churn_df.hist(figsize = (15,15))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kObO1uuU_teC"
      },
      "source": [
        "O Gráfico de Densidade, ou Density Plot, é bem parecido com o histograma, mas com uma visualização um pouco diferente. Com ele, pode ser mais fácil identificar a distribuição do atributos do dataset. Assim como fizemos com o histograma, vamos criar um density plot para cada atributo do dataset.\n",
        "\n",
        "Veremos que muitos dos atributos têm uma distribuição distorcida. Uma transformação como a Box-Cox, que pode aproximar a distribuição de uma Normal, pode ser útil neste caso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHUaJJNK_tji"
      },
      "outputs": [],
      "source": [
        "# Density Plot\n",
        "churn_df.plot(kind = 'density', subplots = True, sharex = False, figsize = (15,10))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08IaXUcCxuj6"
      },
      "source": [
        "Vamos agora trabalhar com boxplots. No **boxblot**, a linha no centro (vermelha) representa o valor da mediana (segundo quartil ou p50). A linha abaixo é o 1o quartil (p25) e a linha acima o terceiro quartil (p75). O boxplot ajuda a ter uma ideia da dispersão dos dataset e os possíveis outliers.\n",
        "\n",
        "*OBS: Se um ponto do dataset é muito distante da média (acima de 3 desvios padrão da média), pode ser considerado outlier.*\n",
        "\n",
        "Nos gráficos bloxplot, veremos que a dispersão dos atributos do dataset é bem diferente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ctc5ftKgxurF"
      },
      "outputs": [],
      "source": [
        "# Boxplot\n",
        "churn_df.plot(kind = 'box', subplots = True, layout = (3,3), sharex = False, sharey = False, figsize = (15,10))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMMGbjYbAe-3"
      },
      "source": [
        "### 3.3. Visualizações Multimodais\n",
        "\n",
        "Ao visualizar as correlações entre os atributos através da matriz de correlação, perceberemos que parece haver alguma estrutura na ordem dos atributos. O azul ao redor da diagonal sugere que os atributos que estão próximos um do outro são geralmente mais correlacionados entre si. Os vermelhos também sugerem alguma correlação negativa moderada, a medida que os atributos \n",
        "\n",
        "Vamos agora verificar a covariância entre as variáveis numéricas do dataset. A **covariância** representa como duas variáveis numéricas estão relacionadas. Existem várias formas de calcular a correlação entre duas variáveis, como por exemplo, o coeficiente de correlação de Pearson, que pode ser:\n",
        "* Próximo de -1 : há uma correlação negativa entre as variáveis, \n",
        "* Próximo de +1: há uma correlação positiva entre as variáveis. \n",
        "* 0: não há correlação entre as variáveis.\n",
        "\n",
        "<i>OBS: Esta informação é relevante porque alguns algoritmos como regressão linear e regressão logística podem apresentar problemas de performance se houver atributos altamente correlacionados. Vale a pena consultar a documentação do algoritmo para verificar se algum tipo de tratamento de dataset é necessário.</i>\n",
        "\n",
        "Falamos anteriormente da importância da correlação entre os atributos, e agora iremos visualizar esta informação em formato gráfico. A **matriz de correlação** exibe graficamente a correlação entre os atributos numéricos do dataset.estão mais distantes um do outro na ordenação. \n",
        "\n",
        "O código a seguir exibe a matriz de correlação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB612g0aAfE6"
      },
      "outputs": [],
      "source": [
        "# Matriz de Correlação com Matplotlib Seaborn\n",
        "interval_cols = ['tenure', 'monthly_charges', 'total_charges']\n",
        "\n",
        "phik_overview_df = churn_df.phik_matrix(interval_cols=interval_cols)\n",
        "phik_overview_df.sort_values(by=\"churn\", inplace=True, axis=0)\n",
        "phik_overview_df.sort_values(by=\"churn\", inplace=True, axis=1)\n",
        "sorted_phik_df = churn_df[ phik_overview_df.index ]\n",
        "phik_overview_df = sorted_phik_df.phik_matrix(interval_cols=interval_cols)\n",
        "display(phik_overview_df);\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42tB0wH2ol3w"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "plot_correlation_matrix(\n",
        "    phik_overview_df.values\n",
        ",   x_labels          = phik_overview_df.columns\n",
        ",   y_labels          = phik_overview_df.index\n",
        ",   vmin              = 0\n",
        ",   vmax              = 1\n",
        ",   color_map         = 'RdBu_r'\n",
        ",   title             = r'Correlação $\\phi_K$'\n",
        ",   figsize=(25,25)\n",
        ");\n",
        "plt.tight_layout();\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4HYrNIc0lDg"
      },
      "source": [
        "O coeficiente de correlação Phi K consegue calcular as correlações entre variáveis categóricas e numéricas analisando o efeito que as variáveis categóricas possuem sobre as variáveis numéricas intervalares.\n",
        "\n",
        "Diferente de outros algoritmos de para cálculo de correlação, ele consegue capturar de forma adequada relacionamentos não lineares entre variáveis, mas não consegue indicar a direção do relacionamento, apenas se ele existe e sua intensidade, pois os dados retornados variam entre um intervalo entre 0 e 1.\n",
        "\n",
        "Por ser um algoritmo ainda novo, a interpretação da intensidade dos relacionamentos pode ser um tanto dificultada. Portanto, as linhas e colunas do dataframe contendo as correlações foram ordenados com base na intensidade do relacionamento com a variável target **churn**. Nesse sentido, as constatações anteriormente realizadas via análise das tabulações cruzadas com a variável target são reforçadas/confirmadas.\n",
        "\n",
        "Ignorando a primeira coluna/linha (por tratar-se da própria variável target após a ordenação citada acima), podemos verificar que as variáveis que possuem a maior correlação com a mesma são:\n",
        "\n",
        "- **tenure** indicando que os clientes mais novos possuem maior rotatividade e que, com o passar do tempo, tendem a se fidelizar, conforme análise anterior.\n",
        "\n",
        "- **monthly_charges** indicando que os clientes que contratam os serviços mais caros (internet de fibra ótica) tendem a ter a maior rotatividade e que clientes que possuem apenas planos telefônicos, tendem a apresentar menor rotatividade (conforme análise anterior).\n",
        "\n",
        "- **payment_method** indicando que os clientes que realizam pagamento via cheques eletrônicos(depósitos diretos) são os que apresentam a maior rotatividade(conforme análise anterior), possivelmente ligado a facilidade na qual os pagamentos podem ser sustados sem necessidade de contato com a operadora.\n",
        "\n",
        "Existe uma alta correlação entre os diferentes serviços oferecidos e uma correlação dos mesmos com os valores mensais cobrados. Essa relação possivelmente pode ser explicada sobre o efeito que a contratação desses serviços possuem sobre o valor da mensalidade. Nesse sentido, é difícil interpretar as correlações reportadas por falta de uma intuição adequada sobre o seu significado, efeitos e características.\n",
        "\n",
        "Mais detalhes sobre o coeficiente Phi K podem ser encontrados no link abaixo, inclusive o artigo no qual o mesmo foi apresentado.\n",
        "\n",
        "https://towardsdatascience.com/phik-k-get-familiar-with-the-latest-correlation-coefficient-9ba0032b37e7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mE4-PIaTAfKX"
      },
      "source": [
        "## 4. Pré-Processamento de dados\n",
        "\n",
        "Nesta etapa, poderíamos realizar diversas operações de preparação de dados, como por exemplo, tratamento de valores missings (faltantes), limpeza de dados, transformações como one-hot-encoding, seleção de características (feature selection), entre outras não mostradas neste notebook. Lembre-se de não criar uma versão padronizada/normalizada dos dados neste momento (apesar de serem operações de pré-processamento) para evitar o Data Leakage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REo1UJQZLuFV"
      },
      "source": [
        "### 4.1. Tratamento das variáveis categóricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhAz17RD6YP4"
      },
      "source": [
        "Dado o elevado número de variáveis categóricas presentes no data set, iremos adotar a estratégia de **Dummy Encoding** para tranformar os dados categóricos em numéricos e posteriormente removendo as colunas redundantes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6XI8XsaFWNd"
      },
      "outputs": [],
      "source": [
        "# transformando variáveis booleanas em numéricas (dummy encoding não é necessário)\n",
        "for feature in BOOLEAN_FEATURES:\n",
        "  churn_df[feature] = churn_df[feature].map(BOOLEAN_MAP)\n",
        "\n",
        "# realizando o dummy encoding usando pandas\n",
        "dummy_df = pd.get_dummies(\n",
        "    data        = churn_df[CATEGORICAL_FEATURES]\n",
        ",   prefix      = CATEGORICAL_FEATURES\n",
        ",   prefix_sep  = \"=\"\n",
        ")\n",
        "\n",
        "# concatenando as variáveis boleanas, categóricas codificads, numéricas e variável target num novo dataset\n",
        "churn_df = pd.concat([\n",
        "    churn_df[ BOOLEAN_FEATURES ]\n",
        ",   dummy_df\n",
        ",   churn_df[ NUMERICAL_FEATURES ]\n",
        ",   churn_df[ TARGET_VARIABLE ]\n",
        "], axis=1)\n",
        "\n",
        "# removendo variáveis equivalentes internet_service=No = 1\n",
        "del churn_df[\"device_protection=No internet service\"]\n",
        "del churn_df[\"streaming_tv=No internet service\"]\n",
        "del churn_df[\"tech_support=No internet service\"]\n",
        "del churn_df[\"online_backup=No internet service\"]\n",
        "del churn_df[\"streaming_movies=No internet service\"]\n",
        "del churn_df[\"online_security=No internet service\"]\n",
        "\n",
        "# removendo variáveis equivalentes phone_service=0\n",
        "del churn_df[\"multiple_lines=No phone service\"]\n",
        "\n",
        "# removendo variáveis codificada tornadas redundantes pelas deleções acima\n",
        "del churn_df[\"multiple_lines=No\"]\n",
        "del churn_df[\"online_security=No\"]\n",
        "del churn_df[\"online_backup=No\"]\n",
        "del churn_df[\"device_protection=No\"]\n",
        "del churn_df[\"tech_support=No\"]\n",
        "del churn_df[\"streaming_tv=No\"]\n",
        "del churn_df[\"streaming_movies=No\"]\n",
        "del churn_df[\"internet_service=No\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUShoE8NOLFD"
      },
      "source": [
        "As colunas resultantes foram renomeadas para melhorar a legibilidade do dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3IdBUbhKBjY"
      },
      "outputs": [],
      "source": [
        "new_column_names = {\n",
        "    'multiple_lines=Yes'                       : 'multiple_lines'\n",
        ",   'internet_service=DSL'                     : 'dsl'\n",
        ",   'internet_service=Fiber optic'             : 'fiber_optic'\n",
        ",   'online_security=Yes'                      : 'online_security'\n",
        ",   'online_backup=Yes'                        : 'online_backup'\n",
        ",   'device_protection=Yes'                    : 'device_protection'\n",
        ",   'tech_support=Yes'                         : 'tech_support'\n",
        ",   'streaming_tv=Yes'                         : 'streaming_tv'\n",
        ",   'streaming_movies=Yes'                     : 'streaming_movies'\n",
        ",   'contract=Month-to-month'                  : 'monthly_contract'\n",
        ",   'contract=One year'                        : 'one_year_contract'\n",
        ",   'contract=Two year'                        : 'two_year_contract'\n",
        ",   'payment_method=Bank transfer (automatic)' : 'bank_transfer'\n",
        ",   'payment_method=Credit card (automatic)'   : 'credit_card'\n",
        ",   'payment_method=Electronic check'          : 'electronic_check'\n",
        ",   'payment_method=Mailed check'              : 'mailed_check'\n",
        "}\n",
        "churn_df.rename(columns=new_column_names, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEHvgM8jPOtK"
      },
      "source": [
        "## 4.2 Feature Engineering\n",
        "\n",
        "Foi verificado anteriormente que as variáveis **tenure**, **monthly_charges** e **payment_method** possuiam correlação significativa com a variável target.\n",
        "\n",
        "As três variáveis serão combinadas para tentar criar variáveis com melhor poder explicativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK704R1D5kQL"
      },
      "outputs": [],
      "source": [
        "orig_df = churn_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_QC3Y2WEH4o"
      },
      "outputs": [],
      "source": [
        "\n",
        "churn_df = orig_df.copy()\n",
        "\n",
        "rows, cols = churn_df.shape\n",
        "# client factor\n",
        "# A análise das tabulações cruzadas revelou que a existência de parceiro e dependentes tendem a fidelizar o cliente.\n",
        "# Em contrapartida, observou-se que clientes na terceira idade proporcionalmente tendem a cancelar os serviços\n",
        "# de maneira mais frequente.\n",
        "# A expectativa é de que quanto maior for o client_factor, maior a probabilidade de que ele venha a cancelar o seu contrato\n",
        "noise_term = np.random.normal(loc=0.0, scale=0.1, size=rows)\n",
        "churn_df[\"client_factor\"] = (\n",
        "    np.exp(churn_df[\"senior_citizen\"]) # senior_citizen=1 piora as p\n",
        "+   np.exp(np.abs(1-churn_df[\"partner\"]))\n",
        "+   np.exp(np.abs(1-churn_df[\"dependents\"]))\n",
        ") / 3.0 + noise_term\n",
        "\n",
        "# internet factor\n",
        "noise_term = np.random.normal(loc=0.0, scale=0.1, size=rows)\n",
        "churn_df[\"internet_factor\"] = (\n",
        "    np.exp(np.abs(1-churn_df[\"tech_support\"])) \n",
        "+   np.exp(np.abs(1-churn_df[\"online_security\"])) \n",
        "+   np.exp(churn_df[\"dsl\"]) \n",
        "+   np.exp(churn_df[\"fiber_optic\"])\n",
        ") / 4.0 + noise_term\n",
        "\n",
        "# financial factor\n",
        "noise_term = np.random.normal(loc=0.0, scale=0.1, size=rows)\n",
        "churn_df[\"financial_factor\"] = (\n",
        "    np.exp(churn_df[\"monthly_contract\"]) \n",
        "+   np.exp(churn_df[\"electronic_check\"])\n",
        "+   np.exp(churn_df[\"paperless_billing\"])\n",
        ") / 3.0 + noise_term\n",
        "\n",
        "noise_term = np.random.normal(loc=0.0, scale=0.1, size=rows)\n",
        "churn_df[\"multi_factor\"] = (\n",
        "    np.exp(churn_df[\"senior_citizen\"]) # senior_citizen=1 piora as p\n",
        "+   np.exp(np.abs(1-churn_df[\"partner\"]))\n",
        "+   np.exp(np.abs(1-churn_df[\"dependents\"]))\n",
        "+   np.exp(np.abs(1-churn_df[\"tech_support\"])) \n",
        "+   np.exp(np.abs(1-churn_df[\"online_security\"])) \n",
        "+   np.exp(churn_df[\"dsl\"]) \n",
        "+   np.exp(churn_df[\"fiber_optic\"])\n",
        "+   np.exp(churn_df[\"monthly_contract\"]) \n",
        "+   np.exp(churn_df[\"electronic_check\"])\n",
        "+   np.exp(churn_df[\"paperless_billing\"])\n",
        ") / 10.0 + noise_term\n",
        "\n",
        "churn_df[\"churn\"] = churn_df.pop(\"churn\")\n",
        "churn_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhFsh3osRHDM"
      },
      "outputs": [],
      "source": [
        "scatter_vars = [\n",
        "  \"client_factor\", \n",
        "  \"internet_factor\", \n",
        "  \"financial_factor\", \n",
        "  \"multi_factor\", \n",
        "  \"tenure\",\n",
        "  \"monthly_charges\",\n",
        "  \"total_charges\"\n",
        "]\n",
        "\n",
        "sns.pairplot(\n",
        "    churn_df\n",
        ",   diag_kind = \"hist\"    \n",
        ",   vars      = scatter_vars\n",
        ",   hue       = \"churn\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoLQBjKl9JVD"
      },
      "source": [
        "### 4.3. Separação em conjunto de treino e conjunto de teste\n",
        "\n",
        "Para evitar que os processo de treino do modelo resulte em um modelo sobreajustado aos dados (overfitting) é importante testar o modelo com dados ainda não vistos. Modelos sobreajustados tendem a ter uma performance pior estes dados, sendo isso um forte indicativo de que o modelo não foi capaz de generalizar seu poder preditivo.\n",
        "\n",
        "É uma boa prática usar um conjunto de teste, uma amostra dos dados que não será usada para a construção do modelo, mas somente no fim do projeto para confirmar a precisão do modelo final.\n",
        "\n",
        "Trata-se de um boa prática usada para avaliação sistemática da performance do modelo.\n",
        "\n",
        "Usaremos 80% do conjunto de dados para modelagem e treino, e guardaremos 20% para teste, usando a estratégia train-test-split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gscOx9Fpird"
      },
      "source": [
        "## 4.3 Normalização\n",
        "\n",
        "Dado as significativas diferenças entre as escalas das variáveis, a ausência de outliers presentes no conjunto de dados e a presença de variáveis numéricas geradas a partir de dados categóricos usando dummy encoding(representadas como 0's ou 1's), o conjunto de dados será tratado usando o procedimento conhecido como normalização. A normalização irá sempre mapear o intervalo de valores observados nas variávéis para o intervalo entre 0 e 1.\n",
        "\n",
        "A normalização feita abaixo precisará ser posteriormente refeita no pipeline, pois a separação do conjunto de dados de treino e teste ainda não foi realizada, o que poderia ser caracterizado como um data leakage da variável target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEiAm3LEAfPt"
      },
      "outputs": [],
      "source": [
        "all_but_target = churn_df.columns.difference([TARGET_VARIABLE])\n",
        "X = churn_df[all_but_target].values\n",
        "y = churn_df[TARGET_VARIABLE].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X\n",
        ",   y\n",
        ",   test_size     = TEST_PCT_SIZE\n",
        ",   shuffle       = True\n",
        ",   random_state  = RANDOM_STATE\n",
        ",   stratify      = y # com estratificação\n",
        ")\n",
        "\n",
        "X_train_df = pd.DataFrame(X_train, columns=all_but_target)\n",
        "X_test_df = pd.DataFrame(X_test, columns=all_but_target)\n",
        "y_train_df = pd.DataFrame(y_train, columns=[TARGET_VARIABLE])\n",
        "y_test_df = pd.DataFrame(y_test, columns=[TARGET_VARIABLE])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2OGe0DtAfU4"
      },
      "source": [
        "## 5. Modelos de Classificação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwHzQpbX9QQh"
      },
      "source": [
        "### 5.1. Criação e avaliação de modelos: linha base\n",
        "\n",
        "Não sabemos de antemão quais modelos performarão bem neste conjunto de dados. Assim, usaremos a validação cruzada 10-fold (já detalhada anteriormente) e avaliaremos os modelos usando a métrica de acurácia. Vamos inicialmente configurar os parâmetros de número de folds e métrica de avaliação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Foy4MFlSAfaK"
      },
      "outputs": [],
      "source": [
        "# Parâmetros e partições da validação cruzada estratificada\n",
        "kfold = StratifiedKFold(\n",
        "    n_splits      = K_FOLDS\n",
        ",   shuffle       = True\n",
        ",   random_state  = RANDOM_STATE\n",
        ") \n",
        "# configura pandas para exibição de apenas duas casas decimais nas variaveis\n",
        "pd.set_option('display.float_format', lambda x: '%0.3f' % x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8eH2qCB7Glh"
      },
      "outputs": [],
      "source": [
        "# Configuração do pipeline\n",
        "\n",
        "# Os transformadores numéricos são utilizado spara processamento de todas as variáveis não categóricas.\n",
        "numeric_transformer = Pipeline([\n",
        "  (\"scaler\", StandardScaler())    \n",
        "])\n",
        "\n",
        "column_transformer = ColumnTransformer(\n",
        "  transformers = [\n",
        "    (\"num\", numeric_transformer, NUMERICAL_FEATURES_AFTER_FEAT_ENG)\n",
        "  ],\n",
        "  # importante usar passthrough quando nem todos os atributos forem processados\n",
        "  remainder=\"passthrough\" \n",
        ")\n",
        "\n",
        "# Este pipelie será ajustado diversas vezes durante o processo de otimização dos hiper parâmetros.\n",
        "pipeline = Pipeline([\n",
        "    # a primeira fase consiste no pré-processamento das variáveis numéricas\n",
        "    (\"feature_scaling\", column_transformer),\n",
        "    # redução de dimesionalidade\n",
        "    (\"reduce_dim\", PCA()),\n",
        "    # O algoritmo de regressão e seus parâmetros serão configurados via gridsearch\n",
        "    (\"classifier\", SVC())\n",
        "])\n",
        "\n",
        "stk_pipeline = Pipeline([\n",
        "    # O algoritmo de regressão e seus parâmetros serão configurados via gridsearch\n",
        "    (\"classifier\", SVC())                         \n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_hyperparameters(param_grid, num_iter, algo):\n",
        "  model_settings = MODEL_SETTINGS[algo]\n",
        "  if model_settings[\"train\"]:\n",
        "    grid = RandomizedSearchCV(\n",
        "        estimator           = pipeline\n",
        "    ,   param_distributions = param_grid\n",
        "    ,   scoring             = SCORING_METRIC\n",
        "    ,   cv                  = kfold\n",
        "    ,   n_iter              = num_iter\n",
        "    ,   return_train_score  = RETURN_TRAIN_SCORE\n",
        "    )\n",
        "    grid.fit(X_train_df, y_train_df)\n",
        "    save_model(grid, model_settings[\"model_file\"])\n",
        "  else:\n",
        "    grid = read_model(model_settings[\"model_file\"])\n",
        "  print(f\"Melhor ROCAUC: {grid.best_score_}\")\n",
        "  print(\"Melhor estimador -> \")\n",
        "  pprint(grid.best_params_)\n",
        "  results_df = pd.DataFrame(grid.cv_results_)\n",
        "  results_df.to_excel(OUTPUT_TRAINING_FILE_TMPLT.replace(\"{algo}\", algo))\n",
        "  return grid, results_df\n",
        "\n",
        "def plot_mean_std(results_df, param_name, new_name):\n",
        "  results_df[new_name] = results_df[param_name]\n",
        "  return sns.scatterplot(\n",
        "      data=results_df\n",
        "  ,   x=\"mean_test_score\"\n",
        "  ,   y=\"std_test_score\"\n",
        "  ,   hue=new_name\n",
        "  )\n",
        "  \n",
        "def show_top_n(results_df, n, column_mapping):\n",
        "  results_df.sort_values(\"mean_test_score\", inplace=True, ascending=False)\n",
        "  columns = {new_name: results_df[col_name] for new_name, col_name in column_mapping.items()}\n",
        "  params_df = pd.DataFrame(columns)\n",
        "  display(params_df.head(n))\n",
        "  return params_df\n",
        "\n",
        "def report_results(grid):\n",
        "  y_train_hat         = grid.best_estimator_.predict(X_train_df)\n",
        "  y_test_hat          = grid.best_estimator_.predict(X_test_df)\n",
        "  \n",
        "  y_train_prob        = grid.best_estimator_.predict_proba(X_train_df)[:,1]\n",
        "  y_test_prob         = grid.best_estimator_.predict_proba(X_test_df)[:,1]\n",
        "  \n",
        "  acc_train_score     = accuracy_score(y_train, y_train_hat)\n",
        "  acc_test_score      = accuracy_score(y_test, y_test_hat)  \n",
        "  \n",
        "  f1_train_score      = f1_score(y_train, y_train_hat)\n",
        "  f1_test_score       = f1_score(y_test, y_test_hat)\n",
        "  \n",
        "  rocauc_train_score  = roc_auc_score(y_train, y_train_prob)\n",
        "  rocauc_test_score   = roc_auc_score(y_test, y_test_prob)\n",
        "  \n",
        "  print(f\"== Test ACC Score     :  {acc_test_score} ===\")\n",
        "  print(f\"== Test F1 Score      :  {f1_test_score} ===\")\n",
        "  print(f\"== Test ROCAUC Score  :  {rocauc_test_score} ===\")\n",
        "  \n",
        "  plot_confusion_matrix(\n",
        "      grid.best_estimator_\n",
        "  ,   X_test_df\n",
        "  ,   y_test_df\n",
        "  )\n",
        "  plt.show()\n",
        "  plot_roc_curve(\n",
        "      grid.best_estimator_\n",
        "  ,   X_test_df\n",
        "  ,   y_test_df\n",
        "  )\n",
        "  plt.show()\n",
        "  print(classification_report(y_test, y_test_hat))\n",
        "\n",
        "def save_model(grid, file_name):\n",
        "  import pickle\n",
        "  data = {\n",
        "    \"best_score_\"     : grid.best_score_\n",
        "  , \"best_params_\"    : grid.best_params_\n",
        "  , \"best_estimator_\" : grid.best_estimator_\n",
        "  , \"cv_results_\"     : grid.cv_results_\n",
        "  , \"grid\"            : grid\n",
        "  }  \n",
        "  with open(file_name, \"wb\") as fh:\n",
        "    pickle.dump(data, fh)\n",
        "\n",
        "def read_model(file_name):\n",
        "  import pickle\n",
        "  with open(file_name, \"rb\") as fh:\n",
        "    data                  = pickle.load(fh)\n",
        "  grid                  = data[\"grid\"]\n",
        "  grid.best_score_      = data[\"best_score_\"]\n",
        "  grid.best_params_     = data[\"best_params_\"]\n",
        "  grid.best_estimator_  = data[\"best_estimator_\"]\n",
        "  grid.cv_results_      = data[\"cv_results_\"]\n",
        "  return grid"
      ],
      "metadata": {
        "id": "h33NPmcHPyKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-8hXNsJ-V91"
      },
      "outputs": [],
      "source": [
        "# Realiza a busca por hiperparâmetros\n",
        "param_grid = [\n",
        "  # Logistic Regression\n",
        "  {\n",
        "    \"feature_scaling__num__scaler\"  : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "    \"reduce_dim\"                    : [\"passthrough\", PCA(n_components=3), PCA(n_components=5), PCA(n_components=10)],\n",
        "    \"classifier\"                    : [LogisticRegression()],\n",
        "    \"classifier__n_jobs\"            : [-1], # all cpus available\n",
        "    \"classifier__penalty\"           : [\"elasticnet\"],\n",
        "    \"classifier__class_weight\"      : [\"balanced\"],\n",
        "    \"classifier__solver\"            : [\"saga\"],\n",
        "    \"classifier__l1_ratio\"          : [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
        "  },\n",
        "  {\n",
        "    \"feature_scaling__num__scaler\"  : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "    \"reduce_dim\"                    : [\"passthrough\", PCA(n_components=3), PCA(n_components=5), PCA(n_components=10)],\n",
        "    \"classifier\"                    : [LogisticRegression()],\n",
        "    \"classifier__n_jobs\"            : [-1], # all cpus available\n",
        "    \"classifier__penalty\"           : [\"none\"],\n",
        "    \"classifier__class_weight\"      : [\"balanced\"],\n",
        "  }  \n",
        "]\n",
        "if QUICK_RUN:\n",
        "  logreg_grid, logreg_results_df =  search_hyperparameters(param_grid, 20, \"logreg\")\n",
        "else:\n",
        "  logreg_grid, logreg_results_df =  search_hyperparameters(param_grid, 200, \"logreg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn3gPwTkQ-PT"
      },
      "outputs": [],
      "source": [
        "plot_mean_std(logreg_results_df, \"param_classifier__penalty\", \"penalty\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os melhores resultados obtidos com a regressão logística obtiveram um score F1 perto de 0.632 com um desvio padrão da métrica calculado aproximadamente 0.02."
      ],
      "metadata": {
        "id": "mpEk6rjXMlMf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrSzAFLnbNk5"
      },
      "outputs": [],
      "source": [
        "column_mapping={\n",
        "    \"mean_score\"      : \"mean_test_score\"\n",
        ",   \"std_score\"       : \"std_test_score\"\n",
        ",   \"l1_ratio\"        : \"param_classifier__l1_ratio\"\n",
        ",   \"penalty\"         : \"param_classifier__penalty\"\n",
        ",   \"feature_scaling\" : \"param_feature_scaling__num__scaler\"\n",
        ",   \"reduce_dim\"      : \"param_reduce_dim\"\n",
        "}\n",
        "logreg_params_df = show_top_n(logreg_results_df, 50, column_mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os melhores resultados obtidos não usaram nenhum tipo de normalização ou padronização dos dados e o uso de redução de dimensionalidade para 5 features via PCA parece ter um efeito positivo na maioria dos casos"
      ],
      "metadata": {
        "id": "EZhtUpAgNjpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_results(logreg_grid)"
      ],
      "metadata": {
        "id": "pYMKdH_yKyx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<<Conclusão>>"
      ],
      "metadata": {
        "id": "DKOO_FqcQ-Jh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-ViG3_TinBw"
      },
      "outputs": [],
      "source": [
        "# Realiza a busca por hiperparâmetros\n",
        "param_grid = [\n",
        "  # KNeighborsClassifier\n",
        "  {\n",
        "    \"feature_scaling__num__scaler\"  : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "    \"reduce_dim\"                    : [\"passthrough\", PCA(n_components=3), PCA(n_components=5), PCA(n_components=10)],\n",
        "    \"classifier\"                    : [KNeighborsClassifier()],\n",
        "    \"classifier__n_jobs\"            : [-1], # all cpus available\n",
        "    \"classifier__algorithm\"         : [\"kd_tree\"],\n",
        "    \"classifier__metric\"            : [\"minkowski\"],\n",
        "    \"classifier__p\"                 : [0.5, 1.0, 1.5, 2.0], # manhattan, euclidean\n",
        "    \"classifier__n_neighbors\"       : [5, 7, 10, 13, 15, 17, 20],\n",
        "    \"classifier__weights\"           : [\"uniform\", \"distance\"]\n",
        "  }  \n",
        "]\n",
        "if QUICK_RUN:\n",
        "  knn_grid, knn_results_df =  search_hyperparameters(param_grid, 20, \"knn\")\n",
        "else:\n",
        "  knn_grid, knn_results_df =  search_hyperparameters(param_grid, 200, \"knn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gWwgsUuk72a"
      },
      "outputs": [],
      "source": [
        "plot_mean_std(knn_results_df, \"param_classifier__n_neighbors\", \"n_neighbors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os melhores resultados obtidos com o algoritmo KNN obtiveram um score F1 inferior a 0.6 com um desvio padrão da métrica calculado em aproximadamente 0.03."
      ],
      "metadata": {
        "id": "jOaFCZpKUJXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_mapping = {\n",
        "  \"mean_score\"      : \"mean_test_score\"\n",
        ", \"std_score\"       : \"std_test_score\"\n",
        ", \"n_neighbors\"     : \"param_classifier__n_neighbors\"\n",
        ", \"minkowski-p\"     : \"param_classifier__p\"\n",
        ", \"weights\"         : \"param_classifier__weights\"\n",
        ", \"feature_scaling\" : \"param_feature_scaling__num__scaler\"\n",
        ", \"reduce_dim\"      : \"param_reduce_dim\"\n",
        "}\n",
        "knn_params_df = show_top_n(knn_results_df, 50, column_mapping)"
      ],
      "metadata": {
        "id": "Hr4fPccXUfA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os melhores resultados obtidos utilizaram-se de 13 ou mair vizinhos e a redução de dimensionalidade PCA não foi utilizada. Os melhores resultados tipicamente utilizaram o parâmetro p da métrica de Minkowski menor do que 2 na maioria dos casos (indicando que a distância euclidiana não é adequada para uso nesse conjunto de dados)."
      ],
      "metadata": {
        "id": "OBRPxoGbVqc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_results(knn_grid)"
      ],
      "metadata": {
        "id": "0VfO1MBCasce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O cômputo do score F1 usando o conjunto de treinamento indica claramente que ocorreu overfitting na parametrização usada para o algoritmo KNN."
      ],
      "metadata": {
        "id": "H5T1m2uqbDxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realiza a busca por hiperparâmetros\n",
        "param_grid = [\n",
        "  # GaussianNB\n",
        "  {\n",
        "    \"feature_scaling__num__scaler\"  : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "    \"reduce_dim\"                    : [\"passthrough\", PCA(n_components=3), PCA(n_components=5), PCA(n_components=10)],\n",
        "    \"classifier\"                    : [GaussianNB()],\n",
        "    \"classifier__var_smoothing\"     : [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
        "  }  \n",
        "]\n",
        "if QUICK_RUN:\n",
        "  nb_grid, nb_results_df =  search_hyperparameters(param_grid, 20, \"nb\")\n",
        "else:  \n",
        "  nb_grid, nb_results_df =  search_hyperparameters(param_grid, 200, \"nb\")"
      ],
      "metadata": {
        "id": "1nS1_SDTbnC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_mean_std(nb_results_df, \"param_classifier__var_smoothing\", \"var_smoothing\")"
      ],
      "metadata": {
        "id": "gWpCsPHedcMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os melhores resultados obtidos com o algoritmo KNN obtiveram um score F1 superior a 0.62 com um desvio padrão da métrica calculado em aproximadamente 0.018"
      ],
      "metadata": {
        "id": "LhbwV41edwzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_mapping = {\n",
        "  \"mean_score\"      : \"mean_test_score\"\n",
        ", \"std_score\"       : \"std_test_score\"\n",
        ", \"var_smoothing\"   : \"param_classifier__var_smoothing\"\n",
        ", \"feature_scaling\" : \"param_feature_scaling__num__scaler\"\n",
        ", \"reduce_dim\"      : \"param_reduce_dim\"\n",
        "}\n",
        "pd.set_option('display.float_format', lambda x: '%0.10f' % x)\n",
        "nb_params_df = show_top_n(nb_results_df, 50, column_mapping)\n",
        "pd.set_option('display.float_format', lambda x: '%0.3f' % x)"
      ],
      "metadata": {
        "id": "MF03SY9Nd-hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O algoritmo de NaiveBayes não possui muitos hiperparâmetros e comporta-se basicamente de maneira indiferente quanto a aplicação ou não do procedimento de normalização ou padronização.\n",
        "\n",
        "A redução de dimensionalidade, via PCA não ajuda em nada no algoritmo.\n",
        "\n",
        "O conjunto de dados, quando inicialmente analisado, parecia ser um bom candidato para aplicação deste algoritmo devido a elevada presença de variáveis categóricas, entretanto, a performance observada do mesmo foi inferior à performance da regressão logística, até o momento o melhor algoritmo observado."
      ],
      "metadata": {
        "id": "CwljibN1fVM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "report_results(nb_grid)"
      ],
      "metadata": {
        "id": "1uBA0VI2gXDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realiza a busca por hiperparâmetros\n",
        "param_grid = [\n",
        "  # DecisionTreeClassifier\n",
        "  {\n",
        "    \"feature_scaling__num__scaler\"  : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "    \"reduce_dim\"                    : [\"passthrough\", PCA(n_components=3), PCA(n_components=5), PCA(n_components=10)],\n",
        "    \"classifier\"                    : [DecisionTreeClassifier()],\n",
        "    \"classifier__class_weight\"      : [None, \"balanced\"],\n",
        "    \"classifier__criterion\"         : [\"gini\", \"entropy\"],\n",
        "    \"classifier__splitter\"          : [\"best\", \"random\"],\n",
        "    \"classifier__max_features\"      : [None, \"auto\", \"sqrt\", \"log2\"],\n",
        "  }  \n",
        "]\n",
        "if QUICK_RUN:\n",
        "  dt_grid, dt_results_df =  search_hyperparameters(param_grid, 20, \"dt\")\n",
        "else:\n",
        "  dt_grid, dt_results_df =  search_hyperparameters(param_grid, 200, \"dt\")"
      ],
      "metadata": {
        "id": "LP4xNuHVhEW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_mean_std(dt_results_df, \"param_classifier__criterion\", \"criterion\")"
      ],
      "metadata": {
        "id": "ZDxhKdrpiyY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os melhores resultados obtidos com o algoritmo de Árvore de Decisões obtiveram um score F1 com cerca de 0.50 com um desvio padrão da métrica calculado em aproximadamente 0.003\n",
        "\n",
        "Surpreendentemente, o algoritmo se comporta pior do que a regressão logística."
      ],
      "metadata": {
        "id": "DgBLcgdSjT6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_mapping = {\n",
        "  \"mean_score\"      : \"mean_test_score\"\n",
        ", \"std_score\"       : \"std_test_score\"\n",
        ", \"feature_scaling\" : \"param_feature_scaling__num__scaler\"\n",
        ", \"reduce_dim\"      : \"param_reduce_dim\"\n",
        ", \"class_weight\"    : \"param_classifier__class_weight\"\n",
        ", \"criterion\"       : \"param_classifier__criterion\"\n",
        ", \"splitter\"        : \"param_classifier__splitter\"\n",
        ", \"max_features\"    : \"param_classifier__max_features\"\n",
        "}\n",
        "dt_params_df = show_top_n(dt_results_df, 50, column_mapping)"
      ],
      "metadata": {
        "id": "wVH5jUw7jUHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2C7w1OSAffj"
      },
      "source": [
        "O algoritmo de Árvore de Decisão comportou-se melhor com o critério *gini* e parece ter se beneficiado do procedimento de redução de dimensionalidade usando PCA. O balanceamento de classes parece ter ajudado também no processo de treino."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report_results(dt_grid)"
      ],
      "metadata": {
        "id": "M8RNTRm_ySr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assim como para o algoritmo KNN, O cômputo do score F1 usando o conjunto de treinamento indica claramente que ocorreu overfitting na parametrização usada para o algoritmo de Árvore de Decisões."
      ],
      "metadata": {
        "id": "56MBOBvNyip8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realiza a busca por hiperparâmetros\n",
        "param_grid = [\n",
        "  # SVC\n",
        "  {\n",
        "    \"feature_scaling__num__scaler\"  : [MinMaxScaler(), StandardScaler()], # SVC precisa ter os argumentos escalonados para uma melhor performance\n",
        "    \"reduce_dim\"                    : [\"passthrough\", PCA(n_components=5), PCA(n_components=10)],\n",
        "    \"classifier\"                    : [SVC(probability=True)],\n",
        "    \"classifier__kernel\"            : [\"linear\",\"rbf\"],\n",
        "    \"classifier__gamma\"             : [\"scale\", \"auto\"],\n",
        "    \"classifier__class_weight\"      : [\"balanced\"],\n",
        "    #\"classifier__C\"                 : [0.0, 0.5, 1.0, 1.5, 2.0],\n",
        "  }  \n",
        "]\n",
        "if QUICK_RUN:\n",
        "  svm_grid, svm_results_df =  search_hyperparameters(param_grid, 5, \"svm\")\n",
        "else:  \n",
        "  svm_grid, svm_results_df =  search_hyperparameters(param_grid, 50, \"svm\")"
      ],
      "metadata": {
        "id": "aYdC7mUI1a93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_mean_std(svm_results_df, \"param_classifier__kernel\", \"kernel\")"
      ],
      "metadata": {
        "id": "mFB9-YBueH_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Surpreendentemente, o SVM com kernel linear se comportou melhor do que o kernel de *radial basis function*, usado por padrão.\n",
        "\n",
        "Os melhores scores F1 obtidos"
      ],
      "metadata": {
        "id": "c6BDanm_ejD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "column_mapping = {\n",
        "  \"mean_score\"      : \"mean_test_score\"\n",
        ", \"std_score\"       : \"std_test_score\"\n",
        ", \"feature_scaling\" : \"param_feature_scaling__num__scaler\"\n",
        ", \"reduce_dim\"      : \"param_reduce_dim\"\n",
        ", \"gamma\"           : \"param_classifier__gamma\"\n",
        ", \"kernel\"          : \"param_classifier__kernel\"\n",
        "}\n",
        "svm_params_df = show_top_n(svm_results_df, 50, column_mapping)"
      ],
      "metadata": {
        "id": "IZzcXDJle8Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_results(svm_grid)"
      ],
      "metadata": {
        "id": "jyF3o6t8oE5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realiza a busca por hiperparâmetros\n",
        "param_grid = [\n",
        "  # AdaBoostClassifier\n",
        "  {\n",
        "    \"feature_scaling__num__scaler\"    : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "    \"reduce_dim\"                      : [\"passthrough\", PCA(n_components=5), PCA(n_components=10)],\n",
        "    \"classifier\"                      : [AdaBoostClassifier()],\n",
        "    \"classifier__n_estimators\"        : [25, 50, 75, 100],\n",
        "    \"classifier__learning_rate\"       : [0.001, 0.01, 0.1, 1.0]\n",
        "  }  \n",
        "]\n",
        "if QUICK_RUN:\n",
        "  ada_grid, ada_results_df =  search_hyperparameters(param_grid, 5, \"ada\")\n",
        "else:  \n",
        "  ada_grid, ada_results_df =  search_hyperparameters(param_grid, 64, \"ada\")"
      ],
      "metadata": {
        "id": "-hBPA0pNCF0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_mapping = {\n",
        "  \"mean_score\"      : \"mean_test_score\"\n",
        ", \"std_score\"       : \"std_test_score\"\n",
        ", \"feature_scaling\" : \"param_feature_scaling__num__scaler\"\n",
        ", \"reduce_dim\"      : \"param_reduce_dim\"\n",
        ", \"n_estimators\"    : \"param_classifier__n_estimators\"\n",
        ", \"learning_rate\"   : \"param_classifier__learning_rate\"\n",
        "}\n",
        "ada_params_df = show_top_n(ada_results_df, 50, column_mapping)"
      ],
      "metadata": {
        "id": "f8ND6aeL5Wsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_results(ada_grid)"
      ],
      "metadata": {
        "id": "8AAbf4Aq5uH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realiza a busca por hiperparâmetros\n",
        "param_grid = [\n",
        "  # GradientBoostingClassifier\n",
        "  {\n",
        "    \"feature_scaling__num__scaler\"    : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "    \"reduce_dim\"                      : [\"passthrough\", PCA(n_components=5), PCA(n_components=10)],\n",
        "    \"classifier\"                      : [GradientBoostingClassifier()],\n",
        "    \"classifier__loss\"                : [\"log_loss\", \"deviance\", \"exponential\"],\n",
        "    \"classifier__n_estimators\"        : [50, 75, 100, 150],\n",
        "    \"classifier__learning_rate\"       : [0.1, 0.3, 0.5, 0.7, 1.0],\n",
        "    \"classifier__max_depth\"           : [3, 5, 10],\n",
        "    \"classifier__max_features\"        : [None, \"sqrt\", \"log2\"]\n",
        "  }  \n",
        "]\n",
        "if QUICK_RUN:\n",
        "  gb_grid, gb_results_df =  search_hyperparameters(param_grid, 10, \"gb\")\n",
        "else:  \n",
        "  gb_grid, gb_results_df =  search_hyperparameters(param_grid, 100, \"gb\")"
      ],
      "metadata": {
        "id": "yKqKwOuvoFeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_mean_std(gb_results_df, \"param_classifier__n_estimators\", \"n_estimators\")"
      ],
      "metadata": {
        "id": "JjDHPzysp7Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_mapping = {\n",
        "  \"mean_score\"      : \"mean_test_score\"\n",
        ", \"std_score\"       : \"std_test_score\"\n",
        ", \"feature_scaling\" : \"param_feature_scaling__num__scaler\"\n",
        ", \"reduce_dim\"      : \"param_reduce_dim\"\n",
        ", \"loss\"            : \"param_classifier__loss\"\n",
        ", \"n_estimators\"    : \"param_classifier__n_estimators\"\n",
        ", \"loss\"            : \"param_classifier__loss\"\n",
        ", \"learning_rate\"   : \"param_classifier__learning_rate\"\n",
        ", \"max_depth\"       : \"param_classifier__max_depth\"\n",
        ", \"max_features\"    : \"param_classifier__max_features\"\n",
        "}\n",
        "gb_params_df = show_top_n(gb_results_df, 50, column_mapping)"
      ],
      "metadata": {
        "id": "dGZuS7FwvnVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_results(gb_grid)"
      ],
      "metadata": {
        "id": "DOxLnaZoxM8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realiza a busca por hiperparâmetros\n",
        "param_grid = [\n",
        "  # RandomForestClassifier\n",
        "  {\n",
        "    \"feature_scaling__num__scaler\"    : [\"passthrough\", MinMaxScaler(), StandardScaler()],\n",
        "    \"reduce_dim\"                      : [\"passthrough\"], #PCA(n_components=5), PCA(n_components=10)],\n",
        "    \"classifier\"                      : [RandomForestClassifier()],\n",
        "    \"classifier__n_estimators\"        : [50, 100, 150],\n",
        "    \"classifier__criterion\"           : [\"gini\", \"entropy\"],\n",
        "    \"classifier__bootstrap\"           : [True, False],\n",
        "    \"classifier__n_jobs\"              : [-1],\n",
        "    \"classifier__class_weight\"        : [\"balanced\", \"balanced_subsample\"],\n",
        "  }\n",
        "]\n",
        "if QUICK_RUN:\n",
        "  rf_grid, rf_results_df =  search_hyperparameters(param_grid, 10, \"rf\")\n",
        "else:\n",
        "  rf_grid, rf_results_df =  search_hyperparameters(param_grid, 100, \"rf\")"
      ],
      "metadata": {
        "id": "RrULJKq0Jfzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_mean_std(rf_results_df, \"param_classifier__n_estimators\", \"n_estimators\")"
      ],
      "metadata": {
        "id": "fi-g1oTpLJtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_mapping = {\n",
        "  \"mean_score\"      : \"mean_test_score\"\n",
        ", \"std_score\"       : \"std_test_score\"\n",
        ", \"feature_scaling\" : \"param_feature_scaling__num__scaler\"\n",
        ", \"reduce_dim\"      : \"param_reduce_dim\"\n",
        ", \"n_estimators\"    : \"param_classifier__n_estimators\"\n",
        ", \"criterion\"       : \"param_classifier__criterion\"\n",
        ", \"bootstrap\"       : \"param_classifier__bootstrap\"\n",
        ", \"class_weight\"    : \"param_classifier__class_weight\"\n",
        "}\n",
        "rf_params_df = show_top_n(rf_results_df, 50, column_mapping)"
      ],
      "metadata": {
        "id": "Tce2BfbwLW21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report_results(rf_grid)"
      ],
      "metadata": {
        "id": "VAifEVk6MCze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "\n",
        "model_settings = MODEL_SETTINGS[\"vt\"]\n",
        "\n",
        "if not model_settings[\"train\"]:\n",
        "    with open(model_settings[\"model_file\"], \"rb\") as fh:\n",
        "      classifier = pickle.load(fh)\n",
        "else:\n",
        "  def compute_score(estimator):\n",
        "    y_train_prob        = grid.best_estimator_.predict_proba(X_train_df)[:,1]\n",
        "    rocauc_train_score  = roc_auc_score(y_train, y_train_prob)\n",
        "    return rocauc_train_score\n",
        "\n",
        "  # Realiza a busca por hiperparâmetros\n",
        "  estimators_weights = [\n",
        "      ( logreg_grid.best_estimator_,  logreg_grid.best_score_ )\n",
        "  ,   ( knn_grid.best_estimator_,     knn_grid.best_score_    )\n",
        "  ,   ( nb_grid.best_estimator_,      nb_grid.best_score_     )\n",
        "  ,   ( svm_grid.best_estimator_,     svm_grid.best_score_    )\n",
        "  ,   ( ada_grid.best_estimator_,     ada_grid.best_score_    )\n",
        "  ,   ( gb_grid.best_estimator_,      gb_grid.best_score_     )\n",
        "  ,   ( rf_grid.best_estimator_,      rf_grid.best_score_     )\n",
        "  ]\n",
        "\n",
        "  best_score      = -99999\n",
        "  best_estimators = None\n",
        "  best_type       = None\n",
        "\n",
        "  for voting_type in [\"hard\"]: # não estou usando \"soft\" voting para permitir a melhor contribuição de vários modelos\n",
        "    for i in [3, 5, 7]:\n",
        "      for comb_estimators_weights in combinations(estimators_weights, i):\n",
        "        # https://stackoverflow.com/questions/13635032/what-is-the-inverse-function-of-zip-in-python\n",
        "        comb_estimators, weights = zip(*comb_estimators_weights)\n",
        "        classifier = EnsembleVoteClassifier(clfs=comb_estimators, weights=weights, voting=voting_type, refit=False)  \n",
        "        classifier.fit(None, y_train_df)\n",
        "        score = compute_score(classifier)\n",
        "        if score > best_score:\n",
        "          best_score      = score\n",
        "          best_estimators = comb_estimators_weights\n",
        "          best_type       = voting_type\n",
        "\n",
        "  comb_estimators, weights = zip(*best_estimators)\n",
        "  classifier = EnsembleVoteClassifier(clfs=comb_estimators, weights=weights, voting=best_type, refit=False)    \n",
        "  classifier.fit(None, y_train_df)\n",
        "  with open(model_settings[\"model_file\"], \"wb\") as fh:\n",
        "    pickle.dump(classifier, fh)\n",
        "  \n",
        "print(f\"best combination of estimators: {classifier.clfs}\")\n",
        "print(f\"estimators weights: {classifier.weights}\")\n",
        "print(f\"voting type: {classifier.voting}\")\n",
        "\n",
        "class PseudoGrid:\n",
        "  def __init__(self, estimator):\n",
        "      self.best_estimator_ = estimator\n",
        "grid = PseudoGrid(classifier)\n",
        "report_results(grid)"
      ],
      "metadata": {
        "id": "7xotUopqGBv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<<CONCLUSÃO>>"
      ],
      "metadata": {
        "id": "7xRA7Pd4fcFe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAhfSnnIAfke"
      },
      "outputs": [],
      "source": [
        "# Lista que armazenará os modelos\n",
        "models = []\n",
        "\n",
        "# Criando os modelos e adicionando-os na lista de modelos\n",
        "models.append(('LR', LogisticRegression(max_iter=200))) \n",
        "models.append(('KNN', KNeighborsClassifier())) \n",
        "models.append(('CART', DecisionTreeClassifier())) \n",
        "models.append(('NB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So5Y1YxLPnQj"
      },
      "source": [
        "Vamos adicionar também os algoritmos de ensemble que estudamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcQEevg7PmwH"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7) # definindo uma semente global\n",
        "\n",
        "# definindo os parâmetros do classificador base para o BaggingClassifier\n",
        "base = DecisionTreeClassifier()\n",
        "num_trees = 100\n",
        "max_features = 3\n",
        "\n",
        "# criando os modelos para o VotingClassifier - TODO: você poderia experimentar outras variações aqui!\n",
        "bases = []\n",
        "model1 = LogisticRegression(max_iter=200)\n",
        "bases.append(('logistic', model1))\n",
        "model2 = DecisionTreeClassifier()\n",
        "bases.append(('cart', model2))\n",
        "model3 = SVC()\n",
        "bases.append(('svm', model3))\n",
        "\n",
        "# Criando os modelos e adicionando-os na lista de modelos\n",
        "models.append(('Bagging', BaggingClassifier(base_estimator=base, n_estimators=num_trees)))\n",
        "models.append(('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features)))\n",
        "models.append(('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)))\n",
        "models.append(('Ada', AdaBoostClassifier(n_estimators=num_trees)))\n",
        "models.append(('GB', GradientBoostingClassifier(n_estimators=num_trees)))\n",
        "models.append(('Voting', VotingClassifier(bases)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eXEfVJCBQj4"
      },
      "source": [
        "Agora vamos comparar os resultados modelos criados, treinando-os com os dados do conjunto de treino e utilizando a técnica de validação cruzada. Para cada um dos modelos criados, executaremos a validação cruzada e, em seguida, exibiremos a acurácia média e o desvio padrão de cada um. Faremos isso tanto para o dataset original quanto para o dataset sem missings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BosB8tgXJN6r"
      },
      "outputs": [],
      "source": [
        "# Aqui iremos armazenar os resultados tanto para o dataset original quanto para o dataset sem missings\n",
        "results = []\n",
        "names = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUUqbS2fBQrd"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7) # definindo uma semente global\n",
        "\n",
        "# Avaliação dos modelos - dataset original\n",
        "\n",
        "for name, model in models:\n",
        "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbWwadhiIZzH"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7) # definindo uma semente global\n",
        "\n",
        "# Avaliação dos modelos - dataset sem missings\n",
        "\n",
        "for name, model in models:\n",
        "    cv_results = cross_val_score(model, X_train_sm, y_train_sm, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
        "    print(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgVTq2DXBQxw"
      },
      "source": [
        "*Dica: organize os resultados numéricos em tabelas, para facilitar a sua comparação.*\n",
        "\n",
        "Estes resultados sugerem que, para ambos os datasets, diversos modelos têm potencial de trazerem bons resultados, porém, vale observar que estes são apenas valores médios de acurácia, sendo também prudente também observar a distribuição dos resultados dos folds da validação cruzada. Faremos isto comparando os modelos usando boxplots. Os 11 primeiros boxplots são referentes ao dataset original e os seguintes, ao dataset com tratamento de missings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUQvttrIBQ3r"
      },
      "outputs": [],
      "source": [
        "# Comparação dos modelos\n",
        "fig = plt.figure(figsize=(15,10)) \n",
        "fig.suptitle('Comparação dos Modelos - Dataset original e com tratamento de missings') \n",
        "ax = fig.add_subplot(111) \n",
        "plt.boxplot(results) \n",
        "ax.set_xticklabels(names) \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n63s9EyxBQ9w"
      },
      "source": [
        "Os resultados mostram algumas diferenças comparando o dataset original e o dataset com tratamento de missings, parecendo, inicialmente, que o dataset com tratamento de missings gerou melhores resultados (especialmente no KNN: observe a distribuição dos valores). Já considerando a mediana da acurácia marcada como a linha laranja do boxplot, os modelos de regressão logística parecem ter alcançado os melhores resultados.\n",
        "\n",
        "A seguir, repetiremos este processo usando uma visão padronizada e outra normalizada do conjunto de dados de treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olo7SPk2BvvW"
      },
      "source": [
        "### 5.2. Criação e avaliação de modelos: dados padronizados e normalizados\n",
        "\n",
        "Como suspeitamos que as diferentes distribuições dos dados brutos possam impactar negativamente a habilidade de alguns modelos, vamos agora experimentar as visões do dataset padronizado e normalizado, comparando com a visão original do dataset, com e tratamento de missings. Na padronização (*StandardScaler*), os dados serão transformados de modo que cada atributo tenha média 0 e um desvio padrão 1; na normalização (*MinMaxScaler*), cada atributo é redimensionado para um novo intervalo entre 0 e 1.\n",
        "\n",
        "Para evitar o vazamento de dados (*data leakage*) nestas transformações, vamos usar pipelines que padronizam os dados e constroem o modelo para cada fold de teste de validação cruzada. Dessa forma, podemos obter uma estimativa justa de como cada modelo com dados padronizados pode funcionar com dados não vistos.\n",
        "\n",
        "*OBS: Repare que neste notebook estamos usando um código mais \"limpo\" do que nos anteriores.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmQbiYQdBRDW"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7) # definindo uma semente global\n",
        "\n",
        "# Aqui iremos armazenar os pipelines e os resultados para todas as visões do dataset\n",
        "pipelines = []\n",
        "results = []\n",
        "names = []\n",
        "\n",
        "\n",
        "# Criando os elementos do pipeline\n",
        "\n",
        "# Algoritmos que serão utilizados\n",
        "reg_log = ('LR', LogisticRegression(max_iter=200))\n",
        "knn = ('KNN', KNeighborsClassifier())\n",
        "cart = ('CART', DecisionTreeClassifier())\n",
        "naive_bayes = ('NB', GaussianNB())\n",
        "svm = ('SVM', SVC())\n",
        "bagging = ('Bag', BaggingClassifier(base_estimator=base, n_estimators=num_trees))\n",
        "random_forest = ('RF', RandomForestClassifier(n_estimators=num_trees, max_features=max_features))\n",
        "extra_trees = ('ET', ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features))\n",
        "adaboost = ('Ada', AdaBoostClassifier(n_estimators=num_trees))\n",
        "gradient_boosting = ('GB', GradientBoostingClassifier(n_estimators=num_trees))\n",
        "voting = ('Voting', VotingClassifier(bases))\n",
        "\n",
        "# Transformações que serão utilizadas\n",
        "standard_scaler = ('StandardScaler', StandardScaler())\n",
        "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
        "\n",
        "\n",
        "# Montando os pipelines\n",
        "\n",
        "# Dataset original\n",
        "pipelines.append(('LR-orig', Pipeline([reg_log]))) \n",
        "pipelines.append(('KNN-orig', Pipeline([knn])))\n",
        "pipelines.append(('CART-orig', Pipeline([cart])))\n",
        "pipelines.append(('NB-orig', Pipeline([naive_bayes])))\n",
        "pipelines.append(('SVM-orig', Pipeline([svm])))\n",
        "pipelines.append(('Bag-orig', Pipeline([bagging])))\n",
        "pipelines.append(('RF-orig', Pipeline([random_forest])))\n",
        "pipelines.append(('ET-orig', Pipeline([extra_trees])))\n",
        "pipelines.append(('Ada-orig', Pipeline([adaboost])))\n",
        "pipelines.append(('GB-orig', Pipeline([gradient_boosting])))\n",
        "pipelines.append(('Vot-orig', Pipeline([voting])))\n",
        "\n",
        "# Padronização do dataset original\n",
        "pipelines.append(('LR-padr', Pipeline([standard_scaler, reg_log]))) \n",
        "pipelines.append(('KNN-padr', Pipeline([standard_scaler, knn])))\n",
        "pipelines.append(('CART-padr', Pipeline([standard_scaler, cart])))\n",
        "pipelines.append(('NB-padr', Pipeline([standard_scaler, naive_bayes])))\n",
        "pipelines.append(('SVM-padr', Pipeline([standard_scaler, svm])))\n",
        "pipelines.append(('Bag-padr', Pipeline([standard_scaler, bagging]))) \n",
        "pipelines.append(('RF-padr', Pipeline([standard_scaler, random_forest])))\n",
        "pipelines.append(('ET-padr', Pipeline([standard_scaler, extra_trees])))\n",
        "pipelines.append(('Ada-padr', Pipeline([standard_scaler, adaboost])))\n",
        "pipelines.append(('GB-padr', Pipeline([standard_scaler, gradient_boosting])))\n",
        "pipelines.append(('Vot-padr', Pipeline([standard_scaler, voting])))\n",
        "\n",
        "# Normalização do dataset original\n",
        "pipelines.append(('LR-norm', Pipeline([min_max_scaler, reg_log]))) \n",
        "pipelines.append(('KNN-norm', Pipeline([min_max_scaler, knn])))\n",
        "pipelines.append(('CART-norm', Pipeline([min_max_scaler, cart])))\n",
        "pipelines.append(('NB-norm', Pipeline([min_max_scaler, naive_bayes])))\n",
        "pipelines.append(('SVM-norm', Pipeline([min_max_scaler, svm])))\n",
        "pipelines.append(('Bag-norm', Pipeline([min_max_scaler, bagging]))) \n",
        "pipelines.append(('RF-norm', Pipeline([min_max_scaler, random_forest])))\n",
        "pipelines.append(('ET-norm', Pipeline([min_max_scaler, extra_trees])))\n",
        "pipelines.append(('Ada-norm', Pipeline([min_max_scaler, adaboost])))\n",
        "pipelines.append(('GB-norm', Pipeline([min_max_scaler, gradient_boosting])))\n",
        "pipelines.append(('Vot-norm', Pipeline([min_max_scaler, voting])))\n",
        "\n",
        "# Executando os pipelines - datasets sem tratamento de missings\n",
        "print(\"-- Datasets SEM tratamento de missings\")\n",
        "for name, model in pipelines:\n",
        "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %.3f (%.3f)\" % (name, cv_results.mean(), cv_results.std()) # formatando para 3 casas decimais\n",
        "    print(msg)\n",
        "\n",
        "# Executando os pipelines - datasets com tratamento de missings\n",
        "print(\"-- Datasets COM tratamento de missings\")\n",
        "for name, model in pipelines:\n",
        "    cv_results = cross_val_score(model, X_train_sm, y_train_sm, cv=kfold, scoring=scoring)\n",
        "    results.append(cv_results)\n",
        "    names.append(name)\n",
        "    msg = \"%s: %.3f (%.3f)\" % (name, cv_results.mean(), cv_results.std()) # formatando para 3 casas decimais\n",
        "    print(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvqaMjLRBRJE"
      },
      "source": [
        "Vamos analisar estes resultados graficamente:\n",
        "\n",
        "*OBS: você pode preferir fazer um experimento com menos variações para comparar melhor os resultados graficamente, ou mesmo incluir neste gráfico algumas linhas verticais para separar as diferentes visões do dataset.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBcjd2uDBROs"
      },
      "outputs": [],
      "source": [
        "# Comparação dos modelos\n",
        "fig = plt.figure(figsize=(25,6))\n",
        "fig.suptitle('Comparação dos modelos - Dataset orginal, padronizado e normalizado, com e sem tratamento de missings') \n",
        "ax = fig.add_subplot(111) \n",
        "plt.boxplot(results) \n",
        "ax.set_xticklabels(names, rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZFd3ZkNCE_F"
      },
      "source": [
        "Neste primeiro experimento, rodamos 66 configurações: 11 diferentes algoritmos e 6 diferentes visões do nosso dataset!\n",
        "\n",
        "Para o dataset **Sem tratamento de missings**, os melhores modelos em termos de acurácia foram: ET-orig (0,779), Bag-orig (0,77), LR-padr\t(0,77), GB-orig\t(0,769) e Bag-padr (0,767). Já para o dataset **Com tratamento de missings**, os melhores modelos foram: Vot-orig (0,774), ET-norm (0,77), SVM-norm (0,766), LR-padr (0,764) e RF-norm (0,764).\n",
        "\n",
        "Vamos agora fazer um **novo experimento**, fazendo o ajuste do SVM e do KNN, variando os seus hiperparâmetros a fim de buscar configurações que possam gerar resultados melhores.\n",
        "\n",
        "*OBS: Você poderia se aprofundar em outros algoritmos também.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-f2vCU5CMmp"
      },
      "source": [
        "### 5.3. Ajuste dos Modelos (pipeline + gridsearch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwaWY9UuxNpA"
      },
      "source": [
        "#### Ajuste do KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vSn0trh9V6E"
      },
      "source": [
        "Vamos começar ajustando parâmetros como o número de vizinhos e as métricas de distância para o KNN. Para tal, tentaremos todos os valores ímpares de k entre 1 a 21 e as métricas de distância euclidiana, manhattan e minkowski. Usando o pipeline, cada valor de k e de distância será avaliado usando a validação cruzada 10-fold no conjunto de dados sem tratamento de missings e com as visões padronizada e normalizada, que mostrou melhores resultados do que os dados originais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBSDgpXNt1Fp"
      },
      "outputs": [],
      "source": [
        "# Tuning do KNN\n",
        "\n",
        "# Baseado em https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html\n",
        "\n",
        "np.random.seed(7) # definindo uma semente global\n",
        "\n",
        "pipelines = []\n",
        "\n",
        "# definindo os componentes do pipeline\n",
        "knn = ('KNN', KNeighborsClassifier())\n",
        "standard_scaler = ('StandardScaler', StandardScaler())\n",
        "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
        "\n",
        "pipelines.append(('knn-orig', Pipeline(steps=[knn]))) # OBS: \"steps=\" é opcional\n",
        "pipelines.append(('knn-padr', Pipeline(steps=[standard_scaler, knn])))\n",
        "pipelines.append(('knn-norm', Pipeline(steps=[min_max_scaler, knn])))\n",
        "\n",
        "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
        "param_grid = {\n",
        "    'KNN__n_neighbors': [1,3,5,7,9,11,13,15,17,19,21],\n",
        "    'KNN__metric': [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
        "}\n",
        "\n",
        "# Dataset sem tratamento de missings\n",
        "for name, model in pipelines:    \n",
        "    # prepara e executa o GridSearchCV\n",
        "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
        "    grid.fit(X_train, y_train)\n",
        "\n",
        "    # imprime a melhor configuração\n",
        "    print(\"Sem tratamento de missings: %s - Melhor: %f usando %s\" % (name, grid.best_score_, grid.best_params_)) \n",
        "\n",
        "    # imprime todas as configurações\n",
        "    #means = grid.cv_results_['mean_test_score']\n",
        "    #stds = grid.cv_results_['std_test_score']\n",
        "    #params = grid.cv_results_['params']\n",
        "    #for mean, stdev, param in zip(means, stds, params):\n",
        "        #print(\"%f (%f): %r\" % (mean, stdev, param))\n",
        "\n",
        "# Dataset com tratamento de missings\n",
        "for name, model in pipelines:    \n",
        "    # prepara e executa o GridSearchCV\n",
        "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
        "    grid.fit(X_train_sm, y_train_sm)\n",
        "\n",
        "    # imprime a melhor configuração\n",
        "    print(\"Com tratamento de missings: %s - Melhor: %f usando %s\" % (name, grid.best_score_, grid.best_params_)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPXRycKOCikN"
      },
      "source": [
        "Os resultados mostram que a melhor configuração encontrada utiliza o dataset com tratamento de missings, com dados padronizados, distância de manhattan e k = 15."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFhy-staCmo8"
      },
      "source": [
        "#### Ajuste do SVM\n",
        "Iremos ajustar dois dos principais hiperparâmetros do algoritmo SVM: o valor de C (o quanto flexibilizar a margem) e o tipo de kernel utilizado. No Scikit-Learn, o padrão para o algoritmo SVM (implementado pela classe SVC) é usar o kernel da Função Base Radial (RBF) e o valor C definido como 1.0. \n",
        "\n",
        "Iremos testar outros valores para estes hiperparâmetros, e cada combinação de valores será avaliada usando a função GridSearchCV, como fizemos anteriormente para o KNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3TRTxr5xmjL"
      },
      "outputs": [],
      "source": [
        "# Tuning do SVM - DEMORA MUITO ESTE BLOCO DE CÓDIGO\n",
        "\n",
        "# Baseado em https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html\n",
        "\n",
        "np.random.seed(7) # definindo uma semente global\n",
        "\n",
        "pipelines = []\n",
        "\n",
        "# definindo os componentes do pipeline\n",
        "svm = ('SVM', SVC())\n",
        "standard_scaler = ('StandardScaler', StandardScaler())\n",
        "min_max_scaler = ('MinMaxScaler', MinMaxScaler())\n",
        "\n",
        "pipelines.append(('svm-orig', Pipeline(steps=[svm]))) # OBS: \"steps=\" é opcional\n",
        "pipelines.append(('svm-padr', Pipeline(steps=[standard_scaler, svm])))\n",
        "pipelines.append(('svm-norm', Pipeline(steps=[min_max_scaler, svm])))\n",
        "\n",
        "# Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
        "param_grid = {\n",
        "    'SVM__C': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0],\n",
        "    'SVM__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "}\n",
        "\n",
        "# Dataset sem tratamento de missings\n",
        "for name, model in pipelines:    \n",
        "    # prepara e executa o GridSearchCV\n",
        "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
        "    grid.fit(X_train, y_train)\n",
        "\n",
        "    # imprime a melhor configuração\n",
        "    print(\"Sem tratamento de missings: %s - Melhor: %f usando %s\" % (name, grid.best_score_, grid.best_params_)) \n",
        "\n",
        "    # imprime todas as configurações\n",
        "    #means = grid.cv_results_['mean_test_score']\n",
        "    #stds = grid.cv_results_['std_test_score']\n",
        "    #params = grid.cv_results_['params']\n",
        "    #for mean, stdev, param in zip(means, stds, params):\n",
        "        #print(\"%f (%f): %r\" % (mean, stdev, param))\n",
        "\n",
        "# Dataset com tratamento de missings\n",
        "for name, model in pipelines:    \n",
        "    # prepara e executa o GridSearchCV\n",
        "    grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
        "    grid.fit(X_train_sm, y_train_sm)\n",
        "\n",
        "    # imprime a melhor configuração\n",
        "    print(\"Com tratamento de missings: %s - Melhor: %f usando %s\" % (name, grid.best_score_, grid.best_params_)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvxU3-Zb8pPH"
      },
      "source": [
        "Podemos ver que mesmo a configuração do SVM que alcançou a maior acurácia não supera a acurácia mais alta que conseguimos até o momento, com ensembles.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9hQXKXWDPTG"
      },
      "source": [
        "**Exercício:** Experimente variar os hiperparâmetros de outros algoritmos para verificar se é possível encontrar uma configuração de modelo que supere os melhoresd resultados até o momento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuUpaYcwDRDt"
      },
      "source": [
        "## 7. Finalização do Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-il0uhS84DW"
      },
      "source": [
        "Analisando os resultados até aqui, verificamos que o modelo que mostrou melhor acurácia média para o problema foi o que usou Extra Trees como algoritmo (apesar de ter um  desvio padrão relativamente alto). Relembrando o Experimento 1 (uma vez que o Experimento 2 não trouxe resultados melhores), nossos resultados foram:\n",
        "\n",
        "*Para o dataset **Sem tratamento de missings**, os melhores modelos em termos de acurácia foram: ET-orig (0,779), Bag-orig (0,77), LR-padr\t(0,77), GB-orig\t(0,769) e Bag-padr (0,767). Já para o dataset **Com tratamento de missings**, os melhores modelos foram: Vot-orig (0,774), ET-norm (0,77), SVM-norm (0,766), LR-padr (0,764) e RF-norm (0,764).*\n",
        "\n",
        "Examinando também o desvio padrão, poderíamos, por exemplo, optar por utilizar o modelo construído com o algoritmo de Regressão Logística, com os dados sem tratamento de missings, visão padronizada. Considerando o dataset \"Sem tratamento de missings\", este modelo ficou na 2a posição em termos de acurácia média, mas com um desvio padrão menor do que o que alcançou a 1a posição. Além disso, explicar como funciona este modelo para os usuários não técnicos tende a ser mais simples.\n",
        "\n",
        "A seguir, finalizaremos este modelo, treinando-o em todo o conjunto de dados de treinamento (sem validação cruzada) e faremos predições para o conjunto de dados de teste que foi separado logo no início do exemplo, a fim de confirmarmos nossas descobertas.\n",
        "\n",
        "Primeiro, iremos realizar a padronização dos dados de entrada. Depois, treinaremos o modelo e exibiremos a acurácia de teste, a matriz de confusão e o relatório de classificação."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbrFxAbSDVIj"
      },
      "outputs": [],
      "source": [
        "# Preparação do modelo\n",
        "scaler = StandardScaler().fit(X_train) # ajuste do scaler com o conjunto de treino\n",
        "rescaledX = scaler.transform(X_train) # aplicação da padronização no conjunto de treino\n",
        "model = LogisticRegression(max_iter=200) # substitua aqui se quiser usar outro modelo\n",
        "model.fit(rescaledX, y_train)\n",
        "\n",
        "# Estimativa da acurácia no conjunto de teste\n",
        "rescaledTestX = scaler.transform(X_test) # aplicação da padronização no conjunto de teste\n",
        "predictions = model.predict(rescaledTestX)\n",
        "print(accuracy_score(y_test, predictions))\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7W7I4j7hDZyE"
      },
      "source": [
        "Por meio do conjunto de teste, verificamos que alcançamos uma acurácia de 77,22% em dados não vistos. Este resultado foi ainda melhor do que a nossa avaliação anterior da regressão logíistica. Valores semelhantes são esperados quando este modelo estiver executando em produção e fazendo predições para novos dados.\n",
        "\n",
        "Vamos agora preparar o modelo para utilização. Para isso, vamos treiná-lo com todo o dataset, e não apenas o conjunto de treino."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGeQHmeg4ziu"
      },
      "outputs": [],
      "source": [
        "# Preparação do modelo com TODO o dataset (e não apenas a base de treino)\n",
        "scaler = StandardScaler().fit(X) # ajuste do scaler com TODO o dataset\n",
        "rescaledX = scaler.transform(X) # aplicação da padronização com TODO o dataset\n",
        "model.fit(rescaledX, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZ-FQWZj_OtQ"
      },
      "source": [
        "## 8. Aplicando o modelo em dados não vistos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ORA5V8f53l_"
      },
      "source": [
        "Agora imagine que chegaram 3 novas instâncias, mas não sabemos a classe de saída. Podemos então aplicar nosso modelo recém-treinado para estimar as classes! Para tal, será necessário antes padronizar os dados (usando a mesma escala dos dados usados treinamento do modelo!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAIp6d9w5QG8"
      },
      "outputs": [],
      "source": [
        "# Novos dados - não sabemos a classe!\n",
        "data = {'preg':  [1, 9, 5],\n",
        "        'plas': [90, 100, 110],\n",
        "        'pres': [50, 60, 50],\n",
        "        'skin': [30, 30, 30],\n",
        "        'test': [100, 100, 100],\n",
        "        'mass': [20.0, 30.0, 40.0],\n",
        "        'pedi': [1.0, 2.0, 1.0],\n",
        "        'age': [15, 40, 40],  \n",
        "        }\n",
        "\n",
        "atributos = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']\n",
        "entrada = pd.DataFrame(data, columns=atributos)\n",
        "\n",
        "array_entrada = entrada.values\n",
        "X_entrada = array_entrada[:,0:8].astype(float)\n",
        "print(X_entrada)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKMSt3P05Qoh"
      },
      "outputs": [],
      "source": [
        "# Padronização nos dados de entrada usando o scaler utilizado em X\n",
        "rescaledEntradaX = scaler.transform(X_entrada)\n",
        "print(rescaledEntradaX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQf_VFWy5Qsm"
      },
      "outputs": [],
      "source": [
        "# Estimativa de classes dos dados de entrada\n",
        "saidas = model.predict(rescaledEntradaX)\n",
        "print(saidas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajCN3RywDdi9"
      },
      "source": [
        "## Resumo\n",
        "\n",
        "Resumidamente, neste exemplo trabalhamos com um problema de classificação binária de ponta a ponta. As etapas abordadas foram:\n",
        "* Definição do problema (Pima Indians Diabetes).\n",
        "* Carga dos dados\n",
        "* Análise  e tratamento dos dados (verificamos que estavam na mesma escala, mas com diferentes distribuições de dados e possíves missings).\n",
        "* Avaliação de modelos de linha base, considerando o dataset original e com tratamento de missings.\n",
        "* Avaliação de modelos com normalização e padronização dos dados.\n",
        "* Ajuste dos modelos, buscando melhorar o KNN e o SVM.\n",
        "* Finalização do modelo (use todos os dados de treinamento e valide usando o conjunto de dados de teste).\n",
        "\n",
        "É importante ressaltar que este exemplo não buscou ser exaustivo, apresentando apenas uma parte dos muitos recursos disponíveis na biblioteca Scikit-Learn. Poderíamos ter testado outras operações de pré-processamento de dados (como feature selection), outros valores de hiperparâmetros e ainda, outros modelos de classificação. Recomendamos que você explore a documentação disponível e incremente este notebook com novas possibilidades."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jz379QwLvKM"
      },
      "source": [
        "## Para saber mais:\n",
        "* Statistical Significance Tests for Comparing Machine Learning Algorithms: https://machinelearningmastery.com/statistical-significance-tests-for-comparing-machine-learning-algorithms/\n",
        "* Hypothesis Test for Comparing Machine Learning Algorithms: https://machinelearningmastery.com/hypothesis-test-for-comparing-machine-learning-algorithms/\n",
        "* How to Calculate Parametric Statistical Hypothesis Tests in Python: https://machinelearningmastery.com/parametric-statistical-significance-tests-in-python/\n",
        "* How to Use Statistical Significance Tests to Interpret Machine Learning Results: https://machinelearningmastery.com/use-statistical-significance-tests-interpret-machine-learning-results/\n",
        "* 17 Statistical Hypothesis Tests in Python (Cheat Sheet): https://machinelearningmastery.com/statistical-hypothesis-tests-in-python-cheat-sheet/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ML - 7. Projeto completo de Classificação Binária - Telco Churn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}